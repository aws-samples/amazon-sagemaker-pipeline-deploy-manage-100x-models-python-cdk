import json
import logging
import os
import tarfile
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import (accuracy_score, roc_auc_score)


def is_safe_member(member):
    # Implement your validation logic here
    if member.name=="xgboost-model":
        return True
    
    
def get_xgboost_model():
    model_dir = '/opt/ml/processing/model'

    model_tar_gz_path = os.path.join(model_dir, 'model.tar.gz')
    with tarfile.open(model_tar_gz_path) as tar:
        for member in tar.getmembers():
            if is_safe_member(member):
                tar.extract(member, path=model_dir)

    print("load the model")
    
    xgb_model = xgb.Booster()
    model_file_path = os.path.join(model_dir, "xgboost-model")
    xgb_model.load_model(model_file_path)
    return xgb_model



def find_optimal_cut_off(target, predicted):
    """
    Function to find the optimal probability threshold of a model for a given dataset

    :param: target: the list of values containing the actual target values. Used for comparison
    :type: target: list
    :param: predicted: the list of predicted probability generated on the test set by the model
    :type: predicted: list
    :return: the optimal threshold value
    :rtype: list
    """
    fpr, tpr, threshold = roc_curve(target, predicted)
    i = np.arange(len(tpr))
    roc = pd.DataFrame(
        {
            "tf": pd.Series(tpr - (1 - fpr), index=i),
            "threshold": pd.Series(threshold, index=i),
        }
    )
    roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]
    return list(roc_t["threshold"])


def auc_scores(y_actual, predicted, pred_prob):
    """
    Function to find the area under the curve from the roc curve generated.

    :param y_actual: the list of values containing the actual target values. Used for comparison
    :type target: list
    :param predicted: the predicted class based on the predicted probability and probability threshold
    :type predicted: list
    :param pred_prob: the predicted probability generated by the model
    :type pred_prob: list
    :return: auc score, false positive rate and true positive rate
    :rtype: float, float, float
    """
    auc_score = roc_auc_score(y_actual, predicted)
    fpr_df, tpr_df, _ = roc_curve(y_actual, pred_prob)
    return auc_score, fpr_df, tpr_df


def computing_metrics(models, X_test, y_test):
    """
    Function to compute and print the various metrics used for model performance analysis

    :param models: The list of trained models
    :type models: list(models)
    :param X_test: The test dataset for which the model predictions are generated
    :type X_test: list
    :param y_test: the actual target values of the test dataset. Used for comparing the predicted values
    :type y_test: list
    :return: the predicted probability scores for every customer in the test dataset.
    This function also prints the other necessary metrics such as precision, optimal threshold, accuracy.
    :rtype: list
    """
    print("Computing Metrics of the models")
    plt.figure(figsize=(12, 6), linewidth=1)
    predicted_proba = []
    for model in models:
        print("Showing result metrics for model:")
        print(type(model).__name__)
        print("Optimal probability threshold: ")
        threshold = find_optimal_cut_off(y_test, model.predict_proba(X_test)[:, 1])
        print(threshold)
        predicted_probs = model.predict_proba(X_test)
        predicted = (predicted_probs[:, 1] >= threshold).astype("int")

        predicted_proba.append(predicted_probs)

        print(accuracy_score(y_test, predicted))
        print(classification_report(y_test, predicted))

        auc, fpr, tpr = auc_scores(y_test, predicted, model.predict_proba(X_test)[:, 1])
        plt.plot(
            fpr, tpr, label=str(type(model).__name__) + "Score " + str(round(auc, 5))
        )
    plt.plot([0, 1], [0, 1], "k--", label="Random: 0.5")
    plt.xlabel("False positive rate")
    plt.ylabel("True positive rate")
    plt.title("ROC Curve")
    plt.legend(loc="best")
    plt.show()
    return predicted_proba


def get_feature_importance(model, X_train):
    """
    Function to find the feature importance values of the different features used in training the model

    :param model: The model for which the feature importance needs to be identified
    :type model: model
    :param X_train: The training dataset used to train the model
    :type X_train: numpy array
    :return: displays the feature importance values for each of the feature along with a chart to visualize it
    :rtype: none
    """

    importances = model.feature_importances_
    forest_importances = pd.Series(importances, index=X_train.columns)
    print(forest_importances)
    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
    fig, ax = plt.subplots()
    forest_importances.plot.bar(yerr=std, ax=ax)
    ax.set_title("Feature importances using MDI")
    ax.set_ylabel("Mean decrease in impurity")
    fig.tight_layout()
    return None


def generate_metrics_table(
    data,
    no_bins,
    target_col,
    probability_col,
    positive_cum_col="Positive_Cumulative_Sum",
    all_cum_col="All_Cumulative_Sum",
    tp="TP",
    fp="FP",
    tn="TN",
    fn="FN",
):
    """
    Generate metrics on a set of data containing both probability column and target
    Example Usage:
    metrics_table = generate_metrics_table(scored_data, 200, 'TARGET_COL', 'SCORES')
    :param data: Scored dataframe with probabilities and target column
    :type data: pd.DataFrame
    :param no_bins: Number of buckets to discretize probabilities into (qcut is used meaning
    that the buckets are equally sized)
    :type no_bins: int
    :param target_col: Column containing whether the record is positive or negative
    :type target_col: str
    :param probability_col: Column containing probabilities
    :type probability_col: str
    :param positive_cum_col: Column with positive records cumulative sum
    :type positive_cum_col: str
    :param all_cum_col: Column with all records cumulative sum
    :type all_cum_col: str
    :param tp: column referring to True Positives
    :type tp: str
    :param fp: column referring to False Positives
    :type fp: str
    :param tn: column referring to True Negatives
    :type tn: str
    :param fn: column referring to False Negatives
    :type fn: str
    :return: DataFrame with performance metrics of model at varying thresholds
    :rtype: pd.DataFrame
    """
    data = bucket_data(data, no_bins, probability_col)
    total_target, total_count = get_totals(data, target_col)
    metric_table = aggregate_data_buckets(data, target_col, probability_col)
    metric_table = calculate_confusion_matrix(
        metric_table, total_count, total_target, positive_cum_col, all_cum_col
    )
    metric_table = calculate_metrics(metric_table, tp, fp, tn, fn)
    return metric_table


def bucket_data(data, no_bins, probability_col):
    """
    Bucket data into equally sized bins based on the probability column
    :param data: Scored dataframe
    :type data: pd.DataFrame
    :param no_bins: Number of bins to split the data into
    :type no_bins: int
    :param probability_col: Column containing the probability from the model
    :type probability_col: str
    :return: Scored dataset with an additional column referring to probability column
    :rtype: pd.DataFrame
    """
    data["bins"] = pd.qcut(
        data[probability_col], q=no_bins, labels=False, duplicates="drop"
    )
    return data


def get_totals(data, target_col):
    """
    Getting total number of records and positive records
    :param data: Scored dataframe
    :type data: pd.DataFrame
    :param target_col: Column referring to target column
    :type target_col: str
    :return: Total Positive Records, Total Count Records
    :rtype: int, int
    """
    total_target = data[target_col].sum()
    total_count = data.shape[0]
    logging.info(f"There are {total_target} positive records")
    logging.info(f"The total number of records is: {total_count}")
    return total_target, total_count


def aggregate_data_buckets(data, target_col, probability_col):
    """
    Aggregate the data into buckets (which are based on probabilities)
    :param data: Scored dataframe
    :type data: pd.DataFrame
    :param target_col: Column referring to target column
    :type target_col: str
    :param probability_col: Column containing the probability from the model
    :type probability_col: str
    :return: Aggregated dataframe that is the base for the rest of the metrics
    :rtype: pd.DataFrame
    """
    metric_table = data.groupby("bins").agg(
        {target_col: ["count", "sum"], probability_col: ["min", "max"]}
    )

    # renaming columns
    metric_table.columns = metric_table.columns.map("_".join)
    metric_table.rename(
        columns={
            target_col + "_" + "count": "Total Records in Bin",
            target_col + "_" + "sum": "Positive Records in Bin",
            probability_col + "_" + "min": "Min Probability Score",
            probability_col + "_" + "max": "Max Probability Score",
        },
        inplace=True,
    )

    metric_table = metric_table.sort_values(
        by=["Min Probability Score"], ascending=False
    )

    metric_table[["All_Cumulative_Sum", "Positive_Cumulative_Sum"]] = metric_table[
        ["Total Records in Bin", "Positive Records in Bin"]
    ].cumsum()
    return metric_table


def calculate_confusion_matrix(
    metric_table,
    total_count,
    total_target,
    positive_cum_col="Positive_Cumulative_Sum",
    all_cum_col="All_Cumulative_Sum",
):
    """
    Generate a confusion matrix per bucket of probabilities
    :param metric_table: Aggregated table based on buckets of the probabilities
    :type metric_table: pd.DataFrame
    :param total_count: Total number of records in the scored dataframe
    :type total_count: int
    :param total_target: Total number of positive records in the scored dataframe
    :type total_target: int
    :param positive_cum_col: Column with positive records cumulative sum
    :type positive_cum_col: str
    :param all_cum_col: Column with all records cumulative sum
    :type all_cum_col: str
    :return: Metrics table with additional columns reflecting the confusion matrix per
    bucket
    :rtype: pd.DataFrame
    """
    metric_table["TP"] = metric_table[positive_cum_col]
    metric_table["FP"] = metric_table[all_cum_col] - metric_table[positive_cum_col]
    metric_table["FN"] = total_target - metric_table["TP"]
    metric_table["TN"] = (
        total_count - metric_table["TP"] - metric_table["FN"] - metric_table["FP"]
    )

    return metric_table


def calculate_metrics(metric_table, tp="TP", fp="FP", tn="TN", fn="FN"):
    """
    Additional metrics to be calculated following the confusion matrix per row
    :param metric_table: Metrics table with additional columns reflecting the confusion matrix per
    bucket
    :type metric_table: pd.DataFrame
    :param tp: column referring to True Positives
    :type tp: str
    :param fp: column referring to False Positives
    :type fp: str
    :param tn: column referring to True Negatives
    :type tn: str
    :param fn: column referring to False Negatives
    :type fn: str
    :return: Metrics table with additional key metrics
    :rtype: pd.DataFrame
    """
    metric_table["Precision"] = metric_table[tp] / (metric_table[tp] + metric_table[fp])
    metric_table["TPR"] = metric_table[tp] / (metric_table[tp] + metric_table[fn])
    metric_table["FPR"] = metric_table[fp] / (metric_table[fp] + metric_table[tn])
    return metric_table


def get_population_percent(population, total_population):
    """
    Calculate the percent of population by performing simple division.

    :param population: The amount of population to be measured
    :type population: int
    :param total_population: Total population
    :type total_population: int
    :returns: Fraction that represents the ratio of population
    :rtype: float
    """
    population_percent = population / total_population
    return population_percent


def get_metrics_table(pred_prob, y_test_actual, n_bins):
    """
    Generates the metrics table containing the probability thresholds for different bins along with the precision
    for each probability threshold

    :param pred_prob: list of predicted probabilities by the model for each data record in the test dataset
    :type pred_prob: list
    :param y_test_actual: actual target value that is used for comparing predicted values to correct values
    :type y_test_actual: list
    :param n_bins: Number of bins to generate different number of probability threshold
    :type n_bins: int
    :returns: a pandas dataframe that contains all the required metrics like probability threshold and precision
    :rtype: pd.dataframe
    """
    dataset_final = pd.DataFrame(
        {"predicted_probabilities": pred_prob, "y_test_actual": y_test_actual},
        columns=["predicted_probabilities", "y_test_actual"],
    )
    metrics_table_var = generate_metrics_table(
        dataset_final, n_bins, "y_test_actual", "predicted_probabilities"
    )
    total_population = max(metrics_table_var["All_Cumulative_Sum"])
    metrics_table_var["Population_percent"] = metrics_table_var[
        "All_Cumulative_Sum"
    ].apply(get_population_percent, total_population=total_population)
    return metrics_table_var


def lr_calculate_feature_importance(model, feature_names):
    """
    Calculate the feature importance based on the logistic regression model and return a DataFrame with the results.

    :param model: the trained logistic regression model
    :type model: sklearn model
    :param feature_names: list of features used in the training data to train the model
    :type feature_names: list
    :returns: Pandas dataframe that contains the feature importance (coefficient) values for each feature
    :rtype:
    """
    importance = abs(model.coef_)[0]
    feature_importance = pd.DataFrame(
        {"Feature": feature_names, "Importance": importance}
    )
    return feature_importance


def export_feature_importance(feature_importance, file_path):
    """
    Export the feature importance DataFrame to an Excel file.

    param feature_importance: the dataframe containing the feature importance values for each feature
    :type feature_importance: pandas dataframe
    :param file_path: The path to the excel file to export the feature importance dataframe
    :type file_path: str
    :returns: none
    :rtype: none
    """
    feature_importance.to_excel(file_path, index=False)
